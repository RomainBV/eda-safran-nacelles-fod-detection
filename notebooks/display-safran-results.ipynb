{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1e374b",
   "metadata": {},
   "source": [
    "# Proof of Concept: FoD Detection in SAFRAN Aircraft Nacelles Using Acoustic Signatures\n",
    "\n",
    "## Objective\n",
    "This notebook presents a proof of concept (PoC) for detecting **Foreign Object Defects (FoD)** in aircraft nacelles **after assembly**. The nacelle is rotated during testing, and the goal is to **detect and classify impact noises** while **minimizing false positives**.\n",
    "\n",
    "## Detection Challenges\n",
    "FoD are categorized into three difficulty levels based on their acoustic profile:\n",
    "\n",
    "- **Obvious noises**: e.g., tools (cutters, brushes) falling\n",
    "- **Subtle noises**: e.g., rivets dropping\n",
    "- **Silent or near-inaudible**: e.g., chips, tape fragments\n",
    "\n",
    "## Approach\n",
    "We apply a PCA-based anomaly detection framework relying on audio indicators, divided into two main feature categories:\n",
    "\n",
    "1. **Impulse Noise Detection**:\n",
    "   - **Spectral Crest**  \n",
    "   - **Temporal Kurtosis**  \n",
    "   - **Spectral Flux**\n",
    "\n",
    "2. **Ultrasonic Impact Energy**:\n",
    "   - Captures high-frequency characteristics to help discriminate between object types\n",
    "\n",
    "The methodology aims to combine these indicators to highlight **anormal acoustic events** corresponding to potential FoD, while maintaining a low false positive rate.\n",
    "\n",
    "[Related deliverables]{https://drive.google.com/file/d/1r5iWEC_TMMzrGO0J_41L4Kg1oS_k__O1/view?usp=drive_link}\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook includes data exploration, feature computation, PCA modeling.*\n",
    "\n",
    "\n",
    "\n",
    "## Python Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82145266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports standards ---\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Imports scientifiques ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde, chi2\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# --- Machine Learning / Préprocessing ---\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Visualisation ---\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.path import Path as MplPath  # évite conflit avec pathlib.Path\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "# --- Traitement géométrique ---\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db098877",
   "metadata": {},
   "source": [
    "## User imputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae98d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paramètres ===\n",
    "path = '/media/rbeauvais/Elements/romainb/2025-n05-Safran-nacelles-FoD/'\n",
    "date = '2025-04-01'\n",
    "channel = 2\n",
    "sensor = 'zoom-f4'\n",
    "sensor_id = ''\n",
    "feature_list = [ 'crest_factor', 'ultrasoundlevel','spectralflux']\n",
    "features_to_display = 'all'\n",
    "background_label = 'rotation'\n",
    "chi_percent = 99\n",
    "x_thresh = 20\n",
    "y_thresh = 10\n",
    "\n",
    "\n",
    "# Paramètres\n",
    "machine_name = ''\n",
    "batch = 'train'  # 'train' Ou 'test'\n",
    "histogram_n_bits = 6\n",
    "n_clusters = 50\n",
    "threshold = 10 # (%)\n",
    "\n",
    "rolling_window = 0.5 # (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b40a1",
   "metadata": {},
   "source": [
    "# Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EllipseCluster:\n",
    "    def __init__(self, centroid, width, height, angle):\n",
    "        self.centroid = centroid  # Centre de l'ellipse\n",
    "        self.width = width        # Largeur (2 * demi-grand axe)\n",
    "        self.height = height      # Hauteur (2 * demi-petit axe)\n",
    "        self.angle = angle        # Angle d'inclinaison en degrés\n",
    "\n",
    "    # Méthode pour créer l'ellipse\n",
    "    def create_ellipse(self):\n",
    "        return Ellipse(xy=(self.centroid[0], self.centroid[1]), \n",
    "                       width=self.width, \n",
    "                       height=self.height, \n",
    "                       angle=self.angle, \n",
    "                       edgecolor='blue', \n",
    "                       fc='None', \n",
    "                       lw=2)\n",
    "\n",
    "    def get_centroid(self):\n",
    "        \"\"\"Récupérer le centroid de l'ellipse.\"\"\"\n",
    "        return self.centroid\n",
    "\n",
    "# Fonction pour sauvegarder les percentiles dans un fichier CSV\n",
    "def save_percentiles_to_csv(percentiles,machine_name, filepath='percentiles.csv'):\n",
    "    df = pd.DataFrame(percentiles).T\n",
    "    df.columns = ['1st_percentile','5th_percentile', '95th_percentile','99th_percentile']\n",
    "    df.index.name = 'feature'\n",
    "    os.makedirs(f'results/{machine_name}', exist_ok=True)\n",
    "    df.to_csv(f'results/{machine_name}/{filepath}')\n",
    "\n",
    "\n",
    "def display_anomaly_detector(results,filtered,labels_pca,label_colors):\n",
    "    \n",
    "    # =========================================\n",
    "    # === Attribution des couleurs par label ==\n",
    "    # =========================================\n",
    "    label_colors['other'] = 'rgba(0,0,0,1)'  # noir opaque\n",
    "    df_binary = pd.DataFrame(results['Label']).copy()  # Conserve uniquement la colonne 'Label'\n",
    "    df_binary['binary'] = results['Label'].isin(['cutter', 'rivets',\"rivets (pas d'impact)\"]).astype(int)\n",
    "\n",
    "    # Masque des anomalies detéctées par la PCA analysis\n",
    "    anomaly_times = filtered.index[labels_pca == 'Anomalie']\n",
    "    anomalie_values = df_binary.loc[anomaly_times]\n",
    "\n",
    "    # 3. Calculer les cas (positifs = anomalies détectées, négatifs = non détectées)\n",
    "    # TP : anomalie détectée et réellement un impact (binary == 1)\n",
    "    vp = ((anomalie_values['binary'] == 1)).sum()\n",
    "\n",
    "    # FP : anomalie détectée mais pas d’impact réel (binary == 0)\n",
    "    fp = ((anomalie_values['binary'] == 0)).sum()\n",
    "\n",
    "    # VN : pas d’anomalie détectée et pas d’impact réel\n",
    "    non_anomaly_times = df_binary.index.difference(anomaly_times)\n",
    "    non_anomaly_values = df_binary.loc[non_anomaly_times]\n",
    "    vn = ((non_anomaly_values['binary'] == 0)).sum()\n",
    "\n",
    "    # FN : pas d’anomalie détectée mais il y avait un impact réel\n",
    "    fn = ((non_anomaly_values['binary'] == 1)).sum()\n",
    "\n",
    "    binary = df_binary['binary'].values\n",
    "    index = df_binary.index\n",
    "\n",
    "    # Détection des fronts montants et descendants\n",
    "    rising_edges = np.where((binary[:-1] == 0) & (binary[1:] == 1))[0] + 1  # +1 pour pointer sur le 1\n",
    "    falling_edges = np.where((binary[:-1] == 1) & (binary[1:] == 0))[0] + 1\n",
    "\n",
    "    # Associer chaque front montant à son front descendant suivant\n",
    "    events = []\n",
    "    j = 0\n",
    "    for start in rising_edges:\n",
    "        while j < len(falling_edges) and falling_edges[j] <= start:\n",
    "            j += 1\n",
    "        if j < len(falling_edges):\n",
    "            end = falling_edges[j]\n",
    "            labels_in_event = df_binary.iloc[start:end+1]['Label'].unique().tolist()\n",
    "            events.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'labels': labels_in_event\n",
    "            })\n",
    "\n",
    "\n",
    "    # Initialiser un compteur\n",
    "    label_counter = Counter()\n",
    "\n",
    "    # Compter chaque label dans chaque événement\n",
    "    for event in events:\n",
    "        label_counter.update(event['labels'])\n",
    "\n",
    "    # Initialiser un compteur par label\n",
    "    detected_by_anomaly = defaultdict(int)\n",
    "\n",
    "    # Vérifier les anomalies dans chaque segment\n",
    "    anomaly_idx_set = set(anomaly_times)  # plus rapide pour les recherches\n",
    "    has_anomaly_per_event = []\n",
    "\n",
    "    for event in events:\n",
    "        start = event['start']\n",
    "        end = event['end']\n",
    "        label = event['labels'][0]  # on suppose que 'label' est inclus dans chaque event\n",
    "        time_range = index[start:end]\n",
    "        has_anomaly = any(t in anomaly_idx_set for t in time_range)\n",
    "        has_anomaly_per_event.append(has_anomaly)\n",
    "        \n",
    "        if has_anomaly:\n",
    "            detected_by_anomaly[label] += 1\n",
    "\n",
    "    for label, count in label_counter.items():\n",
    "        print(f\"{label} : {count} événements\")\n",
    "        print(f\"  → {detected_by_anomaly[label]} détectés comme anomalies\")\n",
    "\n",
    "    print(f\"{sum(has_anomaly_per_event)} événements contiennent au moins une anomalie sur {len(events)}\")\n",
    "    print(f\"Vrais positifs (TP): {vp}\")\n",
    "    print(f\"Faux positifs (FP): {fp}\")\n",
    "    print(f\"Vrais négatifs (VN): {vn}\")\n",
    "    print(f\"Faux négatifs (FN): {fn}\")\n",
    "    print(f\"Taux bonne de détection : {round(100*vp/len(events))} %\")\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    added_legends = set()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_binary.index,\n",
    "        y=df_binary['binary'],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=1),\n",
    "        showlegend=False\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=anomaly_times,\n",
    "        y=anomalie_values,\n",
    "        mode='markers',\n",
    "        marker=dict(size=20, color='red', symbol='circle-open'),\n",
    "        name='Anomalies',\n",
    "        showlegend='Anomalies' not in added_legends\n",
    "    ), row=1, col=1)\n",
    "\n",
    "\n",
    "    # Points colorés par label (hors 'rotation' et 'other')\n",
    "    for label in valid_labels:\n",
    "        df_label = df_binary[df_binary['Label'] == label]\n",
    "        show_legend = label not in added_legends\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_label.index,\n",
    "            y=df_label['binary'],\n",
    "            mode='markers',\n",
    "            marker=dict(color=label_colors[label], size=6, opacity=0.8),\n",
    "            name=label,\n",
    "            showlegend=show_legend\n",
    "        ), row=1, col=1)\n",
    "        if show_legend:\n",
    "            added_legends.add(label)\n",
    "\n",
    "    # === Ajouter les vspan (vrects globaux pour tout le graphique) ===\n",
    "    for label in df_labels['Label'].unique():\n",
    "        sub_df = df_labels[df_labels['Label'] == label]\n",
    "        for _, row in sub_df.iterrows():\n",
    "            fig.add_vrect(\n",
    "                x0=row['Start'],\n",
    "                x1=row['End'],\n",
    "                fillcolor='blue',\n",
    "                opacity=0.5,\n",
    "                line_width=0,\n",
    "                layer='below',\n",
    "                annotation_text=background_label,\n",
    "                annotation_position=\"top left\",\n",
    "                annotation=dict(font_size=10, font_color='blue')\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1500,\n",
    "        height=300,\n",
    "        hovermode=\"x unified\",\n",
    "        title_text=f\"Impact detection after PCA - Features along time (True posifive rate : {round(100*vp/(vp+fp))} %,True negative rate : {round(100*vn/(vn+fn))} %, True detection rate : {round(100*vp/len(events))} %)\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    save_data = {\n",
    "        \"events\" : events,\n",
    "        \"results\": results,\n",
    "        \"filtered\": filtered,\n",
    "        \"labels_pca\": labels_pca,\n",
    "        \"df_labels\": df_labels  \n",
    "    }\n",
    "    with open(f\"results/plot_inputs_ch{channel}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(save_data, f)\n",
    "\n",
    "\n",
    "\n",
    "def build_labels_and_scores(pca_results, inside_points):\n",
    "    labels_pca = []\n",
    "    y_score_pca = []\n",
    "    \n",
    "    inside_points_set = set(map(tuple, inside_points))  # Conversion pour une recherche rapide\n",
    "    \n",
    "    for point in pca_results:\n",
    "        if tuple(point) in inside_points_set:\n",
    "            labels_pca.append('Etat normal')\n",
    "            y_score_pca.append(0)\n",
    "        else:\n",
    "            labels_pca.append('Anomalie')\n",
    "            y_score_pca.append(1)\n",
    "    \n",
    "    return np.array(labels_pca), np.array(y_score_pca)\n",
    "\n",
    "def build_percentile_labels(results, batch, machine_name, percentile_filename=None):\n",
    "    percentiles = {}\n",
    "    labels_all_indicators = []\n",
    "\n",
    "    if batch == 'train':\n",
    "        # Calculate percentiles for each indicator\n",
    "        for key in feature_list:\n",
    "            p1 = np.nanpercentile(results[key].values, 1)\n",
    "            p5 = np.nanpercentile(results[key].values, 5)\n",
    "            p95 = np.nanpercentile(results[key].values, 95)\n",
    "            p99 = np.nanpercentile(results[key].values, 99)\n",
    "            percentiles[key] = (p1, p5, p95, p99)\n",
    "\n",
    "    else:\n",
    "        # Load percentiles from a CSV file\n",
    "        if percentile_filename:\n",
    "            percentiles_df = pd.read_csv(f'results/{machine_name}/{percentile_filename}', index_col=0)\n",
    "            for key in results.keys():\n",
    "                if key in percentiles_df.index:\n",
    "                    row = percentiles_df.loc[key]\n",
    "                    percentiles[key] = (row['1st_percentile'], row['5th_percentile'], row['95th_percentile'], row['99th_percentile'])\n",
    "\n",
    "    # Define labels based on the calculated percentiles\n",
    "    for key in feature_list:\n",
    "        if key in percentiles:\n",
    "            p1, p5, p95, p99 = percentiles[key]\n",
    "            indicator_labels = np.where(\n",
    "                (results[key].values < p1) | (results[key].values > p99),  # 'out' condition\n",
    "                'Anomalie',\n",
    "                np.where(\n",
    "                    ((results[key].values >= p1) & (results[key].values < p5)) | ((results[key].values > p95) & (results[key].values <= p99)),  # 'middle' condition\n",
    "                    'Etat intermédiaire',\n",
    "                    'Etat normal'  # 'in' condition\n",
    "                )\n",
    "            )\n",
    "            labels_all_indicators.append(indicator_labels)\n",
    "\n",
    "    # Combine labels from all indicators into a single label array\n",
    "    if labels_all_indicators:\n",
    "        # Start with a default label\n",
    "        labels = np.full(results[feature_list[0]].shape, 'Etat normal', dtype=object)  # Use the shape of one of the indicators\n",
    "        for indicator_labels in labels_all_indicators:\n",
    "            labels = np.where(indicator_labels == 'Anomalie', 'Anomalie', labels)\n",
    "            labels = np.where(indicator_labels == 'Etat intermédiaire', 'Etat intermédiaire', labels)\n",
    "    else:\n",
    "        labels = np.array([])  # Return an empty array if no indicators\n",
    "\n",
    "    return percentiles, labels\n",
    "\n",
    "def calculate_ellipses(pca_results, labels, centroids, n_clusters,x_thresh,y_thresh):\n",
    "    ellipses = []\n",
    "    chi_square_99 = chi2.ppf(chi_percent/100, 2)  # Valeur critique pour 99.9% de confiance (2D)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        # Ne cnosidérer que les points appartenant au cluster i\n",
    "        cluster_points = pca_results[labels == i]\n",
    "\n",
    "        if len(cluster_points) < 2:\n",
    "            print(f\"Cluster {i+1} ignoré : pas assez de points ({len(cluster_points)}).\")\n",
    "            continue  # Passer au cluster suivant si pas assez de points\n",
    "        \n",
    "        cx, cy = centroids[i]\n",
    "        # Filtrer sur la position des centroïdes hors du scope de l'étude\n",
    "        if x_thresh:\n",
    "            if abs(cx) > x_thresh or abs(cy) > y_thresh:\n",
    "                # ignore cluster whose center is out-of-bounds\n",
    "                continue\n",
    "    \n",
    "        # Calcul de la matrice de covariance pour le cluster\n",
    "        cov_matrix_i = np.cov(cluster_points, rowvar=False)\n",
    "\n",
    "        if cov_matrix_i.shape != (2, 2):\n",
    "            print(f\"Cluster {i+1} ignoré : covariance invalide (shape={cov_matrix_i.shape}).\")\n",
    "            continue  # Passer au cluster suivant si la matrice n'est pas valide\n",
    "        \n",
    "        # Obtenir les valeurs propres et vecteurs propres (axes principaux de l'ellipse)\n",
    "        eigenvalues_i, eigenvectors_i = np.linalg.eigh(cov_matrix_i)\n",
    "\n",
    "        # Demi-grand axe (a) et demi-petit axe (b)\n",
    "        a_i = np.sqrt(eigenvalues_i[1]) * np.sqrt(chi_square_99)  # Demi-grand axe : sqrt de la plus grande valeur propre\n",
    "        b_i = np.sqrt(eigenvalues_i[0]) * np.sqrt(chi_square_99)  # Demi-petit axe : sqrt de la plus petite valeur propre\n",
    "\n",
    "        if a_i < 0.05 or b_i < 0.05:\n",
    "            continue  # Ignorez cette ellipse si l'un des axes est infinitesimal\n",
    "\n",
    "\n",
    "        # Calcul de l'angle d'inclinaison en degrés\n",
    "        theta_i = np.degrees(np.arctan2(eigenvectors_i[1, 1], eigenvectors_i[0, 1]))\n",
    "\n",
    "        ellipse_obj = EllipseCluster(centroid=centroids[i], width=2*a_i, height=2*b_i, angle=theta_i)\n",
    "        ellipses.append(ellipse_obj)\n",
    "\n",
    "    return ellipses\n",
    "\n",
    "# générer les points d'une ellipse\n",
    "def generate_ellipse_points(center, width, height, angle, num_points=200):\n",
    "    t = np.linspace(0, 2 * np.pi, num_points)\n",
    "    a, b = width / 2, height / 2  # Demi-grand axe et demi-petit axe\n",
    "    ellipse = np.array([a * np.cos(t), b * np.sin(t)])  # Points dans le repère local\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "        [np.sin(np.radians(angle)), np.cos(np.radians(angle))]\n",
    "    ])\n",
    "    rotated_ellipse = rotation_matrix @ ellipse  # Rotation de l'ellipse\n",
    "    return rotated_ellipse.T + center  # Translation vers le centre\n",
    "\n",
    "# vérifier si deux ensembles de points se chevauchent\n",
    "def points_overlap(points1, points2):\n",
    "    # Créer des objets Path à partir des ensembles de points\n",
    "    from matplotlib.path import Path\n",
    "    path1 = Path(points1)\n",
    "    path2 = Path(points2)\n",
    "    \n",
    "    # Vérifier si l'un des points de l'ensemble 1 est dans l'ensemble 2 et vice versa\n",
    "    for point in points1:\n",
    "        if path2.contains_point(point):\n",
    "            return True\n",
    "    for point in points2:\n",
    "        if path1.contains_point(point):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# grouper les ellipses chevauchantes\n",
    "def group_ellipses(ellipses):\n",
    "    groups = []  # Liste pour stocker les groupes d'ellipses\n",
    "\n",
    "    for ellipse in ellipses:\n",
    "        added = False\n",
    "        ellipse_points = generate_ellipse_points(ellipse.centroid, ellipse.width, ellipse.height, ellipse.angle)\n",
    "        \n",
    "        for group in groups:\n",
    "            # Calculer les points pour chaque groupe et vérifier si l'ellipse chevauche ce groupe\n",
    "            group_points = []\n",
    "            for e in group:\n",
    "                group_points.extend(generate_ellipse_points(e.centroid, e.width, e.height, e.angle))\n",
    "            \n",
    "            # Vérifier le chevauchement des points\n",
    "            if points_overlap(ellipse_points, np.array(group_points)):\n",
    "                group.append(ellipse)  \n",
    "                added = True\n",
    "                break\n",
    "\n",
    "        if not added:\n",
    "            groups.append([ellipse])  # Si l'ellipse ne chevauche rien, elle forme un nouveau groupe\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def classify_pca_results(pca_results, groups):\n",
    "    inside_points = []\n",
    "    outside_points = []\n",
    "    convex_hulls = []\n",
    "\n",
    "    # Générer les Convex Hulls pour chaque groupe\n",
    "    for i, group in enumerate(groups):\n",
    "        group_points = []\n",
    "        for ellipse in group:\n",
    "            points = generate_ellipse_points(ellipse.centroid, ellipse.width, ellipse.height, ellipse.angle)\n",
    "            if len(points) > 0:\n",
    "                group_points.extend(points)\n",
    "        \n",
    "        group_points_array = np.array(group_points)\n",
    "\n",
    "        # Vérifie si des NaN sont présents\n",
    "        if group_points_array.size == 0 or np.isnan(group_points_array).any():\n",
    "            print(f\"Groupe {i} ignoré : points invalides ou vides.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            hull = ConvexHull(group_points_array)\n",
    "            convex_hulls.append(group_points_array[hull.vertices])\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du calcul du Convex Hull pour le groupe {i} : {e}\")\n",
    "            continue\n",
    "\n",
    "    # Vérifier chaque point par rapport aux Convex Hulls\n",
    "    for point in pca_results:\n",
    "        is_inside = False\n",
    "        for hull_points in convex_hulls:\n",
    "            path = MplPath(hull_points)\n",
    "            if path.contains_point(point):\n",
    "                is_inside = True\n",
    "                break\n",
    "        if is_inside:\n",
    "            inside_points.append(point)\n",
    "        else:\n",
    "            outside_points.append(point)\n",
    "\n",
    "    return np.array(inside_points), np.array(outside_points), convex_hulls\n",
    "\n",
    "def borders(pca_results, labels_percentile,ellipses,rolling_window,block_duration,threshold,batch):\n",
    "    \"\"\"\n",
    "    Identifie les anomalies dans les résultats PCA et prépare les données pour la visualisation.\n",
    "    \n",
    "    pca_results : np.array -> résultats PCA à évaluer\n",
    "    ellipses : list -> liste d'ellipses pour déterminer les frontières\n",
    "    results : dict -> dictionnaire contenant des résultats d'analyse\n",
    "    labels_percentile : array-like -> étiquettes associées aux valeurs de pourcentage\n",
    "    \n",
    "    Returns :\n",
    "        labels_pca : list -> étiquettes 'in' ou 'out' pour chaque point PCA\n",
    "        time : np.array -> vecteur de temps\n",
    "        change_indices : np.array -> indices de changement\n",
    "        indices_flatten : np.array -> indices aplatis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Créer un vecteur de temps (de 0 à la longueur des labels)\n",
    "    time = np.arange(len(labels_percentile)) * block_duration  # Vecteur de temps allant de 0 à (nombre de labels * durée par bloc)\n",
    "    \n",
    "    # Couleurs associées aux labels\n",
    "    label_colors = {'Etat normal': 'darkolivegreen', 'Etat intermédiaire': 'chocolate', 'Anomalie': 'firebrick'}\n",
    "\n",
    "    # Grouper les ellipses\n",
    "    groups = group_ellipses(ellipses)\n",
    "\n",
    "    # Classifier les points\n",
    "    inside_points, _ ,convex_hulls = classify_pca_results(pca_results, groups)\n",
    "    \n",
    "    # Afficher les ellipses et les zones\n",
    "    for i, group in enumerate(groups):\n",
    "        group_points = []\n",
    "        for ellipse in group:\n",
    "            ellipse_points = generate_ellipse_points(ellipse.centroid, ellipse.width, ellipse.height, ellipse.angle)\n",
    "            group_points.extend(ellipse_points)\n",
    "        hull = ConvexHull(np.array(group_points))\n",
    "        hull_points = np.array(group_points)[hull.vertices]\n",
    "        hull_points = np.vstack([hull_points, hull_points[0]])    \n",
    "\n",
    "    labels_pca, y_score_pca = build_labels_and_scores(pca_results, inside_points)\n",
    "\n",
    "    y_score_percentile= []\n",
    "    for label in labels_percentile:\n",
    "        if label == 'Anomalie':\n",
    "            y_score_percentile.append(1)\n",
    "        else:\n",
    "            y_score_percentile.append(0)\n",
    "\n",
    "    # Calcul de la rolling médiane sur 1 min\n",
    "    rolling_percentile = pd.Series(y_score_percentile).rolling(window=int(round(rolling_window/block_duration))).mean().fillna(0)\n",
    "    rolling_pca = pd.Series(y_score_pca).rolling(window=int(round(rolling_window/block_duration))).mean().fillna(0)\n",
    "\n",
    "    \n",
    "    # Initialiser le dictionnaire alarm\n",
    "    alarm = {\n",
    "        'alarm_index_value_percentile': None,\n",
    "        'alarm_index_value_pca': None,\n",
    "        'alarm_index_sample_percentile': None,\n",
    "        'alarm_index_sample_pca': None\n",
    "    }\n",
    "    if batch == 'test':\n",
    "        if any(rolling_percentile > threshold/100):\n",
    "            alarm['alarm_index_value_percentile'] = np.argmax(rolling_percentile > threshold/100)\n",
    "            print(f\"Premier indice où rolling_percentile > {threshold} %: échantillon n° {alarm['alarm_index_sample_percentile']}\")\n",
    "        if any(rolling_pca > threshold/100):\n",
    "            alarm['alarm_index_value_pca'] = np.argmax(rolling_pca > threshold/100)\n",
    "            print(f\"Premier indice où rolling_pca > {threshold} %: échantillon n° {alarm['alarm_index_sample_pca']}\")\n",
    "     \n",
    "\n",
    "    print(f\"taux de valeurs anormales à partir des histogrammes : {round(100*np.mean(y_score_percentile))}%\")\n",
    "    print(f\"taux de valeurs anormales à partir de la PCA : {round(100*np.mean(y_score_pca))}%\")\n",
    "\n",
    "\n",
    "    return labels_pca, time,label_colors,rolling_percentile,rolling_pca,convex_hulls,alarm\n",
    "\n",
    "def display_histograms(data, title_suffix='', percentiles=True):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    pastel_colors = sns.color_palette(\"Set2\", n_colors=len(data)) \n",
    "    features = list(data.keys())\n",
    "    \n",
    "    for i, key in enumerate(features):\n",
    "        plt.subplot(1, len(features), i + 1)\n",
    "        plt.hist(data[key], bins=2**histogram_n_bits, color=pastel_colors[i])\n",
    "        plt.title(f\"{key.capitalize()} histogram\" + title_suffix)\n",
    "        plt.xlabel(key.capitalize())\n",
    "        if percentiles is not None and key in percentiles:\n",
    "            plt.axvline(percentiles[key][0], color='grey', linestyle='--', label='1st percentile')\n",
    "            plt.axvline(percentiles[key][1], color='k', linestyle='--', label='5th percentile')\n",
    "            plt.axvline(percentiles[key][2], color='k', linestyle='--', label='95th percentile')\n",
    "            plt.axvline(percentiles[key][3], color='grey', linestyle='--', label='99th percentile')\n",
    "            if i == 0:\n",
    "                plt.legend(loc='upper right')\n",
    "            plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_rolling_average(rolling_percentile,rolling_pca,time,alarm):\n",
    "    \n",
    "\n",
    "    # Création des sous-graphiques\n",
    "    t0 = time[0]\n",
    "    t_end = time[-1]\n",
    "\n",
    "    _, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)  # 2 subplots superposés\n",
    "\n",
    "    # Plot du rolling_percentile\n",
    "    axs[0].step(time, 100*rolling_percentile)\n",
    "    axs[0].set_title(f'Histogrammes')\n",
    "    axs[0].set_ylabel('%')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_xlim(t0, t_end)  \n",
    "    axs[0].set_ylim(-1, 101)\n",
    "\n",
    "    # Vérifier si alarm['alarm_index_percentile'] est valide\n",
    "    if alarm['alarm_index_value_percentile'] is not None:\n",
    "        alarm_time = time[alarm['alarm_index_value_percentile']]  # L'heure à l'indice de l'alarme\n",
    "        axs[0].axvspan(alarm_time, t_end, color='red', alpha=0.3, label=f\"Échantillon n° {alarm['alarm_index_sample_percentile']}\")\n",
    "        axs[0].legend(loc='upper right')\n",
    "\n",
    "    # Plot du rolling_pca\n",
    "    axs[1].step(time, 100*rolling_pca)\n",
    "    axs[1].set_title(f'PCA')\n",
    "    axs[1].set_xlabel('Time (s)')\n",
    "    axs[1].set_ylabel('%')\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_xlim(t0, t_end)  \n",
    "    axs[1].set_ylim(-1, 101)\n",
    "\n",
    "    # Vérifier si alarm['alarm_index_pca'] est valide\n",
    "    if alarm['alarm_index_value_pca'] is not None:\n",
    "        alarm_time_pca = time[alarm['alarm_index_value_pca']]  # L'heure à l'indice de l'alarme PCA\n",
    "        axs[1].axvspan(alarm_time_pca, t_end, color='red', alpha=0.3, label=f\"Échantillon n° {alarm['alarm_index_sample_pca']}\")\n",
    "        axs[1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_pca(pca_results,convex_hulls,data_scaled,label_colors):\n",
    "    \"\"\"\n",
    "    pca_results : np.ndarray (n_samples, 2)\n",
    "    convex_hulls: list of arrays of shape (m_i, 2)\n",
    "    labels      : array-like of length n_samples with categorical labels\n",
    "    label_colors: dict mapping each label -> a valid CSS color\n",
    "    \"\"\"\n",
    "    x = pca_results[:,0]\n",
    "    y = pca_results[:,1]\n",
    "    xy = np.vstack([x, y])\n",
    "    density = gaussian_kde(xy)(xy)\n",
    "    idx = density.argsort()\n",
    "    x, y, density = x[idx], y[idx], density[idx]\n",
    "    \n",
    "    merged = unary_union([Polygon(h) for h in convex_hulls])\n",
    "    polys = []\n",
    "    if not merged.is_empty:\n",
    "        if merged.geom_type == 'Polygon':\n",
    "            polys = [merged]\n",
    "        else:\n",
    "            polys = list(merged.geoms)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=[\"PCA Density\", \"PCA by Label\"],\n",
    "                        shared_yaxes=True, shared_xaxes=False,\n",
    "                        horizontal_spacing=0.1)\n",
    "    \n",
    "    # density scatter \n",
    "    \n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=y,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=density,\n",
    "            colorscale='Viridis'\n",
    "        ),\n",
    "        showlegend=False  # ← ici à la bonne place, à l'intérieur de go.Scatter\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    for poly in polys:\n",
    "        xh, yh = list(poly.exterior.xy[0]), list(poly.exterior.xy[1])\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=xh, y=yh,\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(255,0,0,0.2)',\n",
    "            line=dict(color='rgba(255,0,0,0)'),\n",
    "            showlegend = False,\n",
    "            hoverinfo='skip',\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    # scatter par label\n",
    "    label_series = data_scaled['Label'].values\n",
    "    for lbl in label_colors.keys():\n",
    "        mask = (label_series == lbl)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pca_results[mask, 0],\n",
    "            y=pca_results[mask, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color=label_colors[lbl]),\n",
    "            name=lbl\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    for poly in polys:\n",
    "        xh, yh = list(poly.exterior.xy[0]), list(poly.exterior.xy[1])\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=xh, y=yh,\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(255,0,0,0.2)',\n",
    "            line=dict(color='rgba(255,0,0,0)'),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False  # déjà affiché à gauche\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"PCA: Density vs Labels\",\n",
    "        width=1200, height=500,\n",
    "        hovermode=\"closest\"\n",
    "    )\n",
    "    fig.update_xaxes(title=\"PC1\", row=1, col=1)\n",
    "    fig.update_xaxes(title=\"PC1\", row=1, col=2)\n",
    "    fig.update_yaxes(title=\"PC2\", row=1, col=1)\n",
    "    fig.update_yaxes(title=\"PC2\", row=1, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def merge_vspans(df_labels, max_gap=pd.Timedelta('5s')):\n",
    "    merged = []\n",
    "    for label, group in df_labels.groupby(\"Label\"):\n",
    "        group = group.sort_values(\"Start\")\n",
    "        current_start = group.iloc[0]['Start']\n",
    "        current_end = group.iloc[0]['End']\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            row = group.iloc[i]\n",
    "            if row['Start'] - current_end <= max_gap:\n",
    "                current_end = max(current_end, row['End'])\n",
    "            else:\n",
    "                merged.append((label, current_start, current_end))\n",
    "                current_start = row['Start']\n",
    "                current_end = row['End']\n",
    "        merged.append((label, current_start, current_end))\n",
    "    return pd.DataFrame(merged, columns=['Label', 'Start', 'End'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a17444",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f174cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "print(path)\n",
    "\n",
    "# === Charger les labels ===\n",
    "label_dir = Path(f\"data/{sensor}/{date}/long_format/labels\")\n",
    "label_files = list(label_dir.rglob(\"labels.csv\"))\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in label_files]\n",
    "df_labels = pd.concat(df_list, ignore_index=True)\n",
    "df_labels['Start'] = pd.to_datetime(df_labels['Start'])\n",
    "df_labels['End'] = pd.to_datetime(df_labels['End'])\n",
    "df_labels = df_labels[df_labels['Label'] == background_label]\n",
    "df_labels = df_labels.drop_duplicates(subset='Start').sort_values(by='Start')\n",
    "df_labels = merge_vspans(df_labels,max_gap=pd.Timedelta(seconds = 30))\n",
    "\n",
    "# === Charger les données stéréo ===\n",
    "if batch == 'train':\n",
    "    df1 = pd.read_csv(f\"results/{sensor}/{sensor_id}/ch{channel}/dataframe_features.csv\", parse_dates=['time'])\n",
    "    df2 = pd.read_csv(f\"results/{sensor}/{sensor_id}/ch{channel+2}/dataframe_features.csv\", parse_dates=['time'])\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "else:\n",
    "    df = pd.read_csv(f\"results/{sensor}/{sensor_id}/ch{channel}/dataframe_features.csv\", parse_dates=['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "if feature_list == 'all':\n",
    "    feature_list = df.columns[df.columns.get_loc('Label') + 1:].tolist()\n",
    "df = df[['Label'] + feature_list]\n",
    "df = df.dropna(subset=feature_list)\n",
    "\n",
    "df['Label'] = df['Label'].replace('impact_inconnu', 'inconnu')\n",
    "df['Label'] = df['Label'].apply(\n",
    "    lambda x: \"rivets (pas d'impact)\" if 'no-impact' in x.lower() else x\n",
    ")\n",
    "df['Label'] = df['Label'].replace('rivets_ch1-2', 'rivets')\n",
    "df['Label'] = df['Label'].replace('rivets_ch-1-2', 'rivets')\n",
    "df['Label'] = df['Label'].replace('rivets_ch-3-4', 'rivets')\n",
    "df['Label'] = df['Label'].replace('rivets_milieu', 'rivets')\n",
    "df['Label'] = df['Label'].replace(' pneumatique', 'pneumatique')\n",
    "# Retouche pour éviter les effets de bords des recordings\n",
    "df.loc[df['ultrasoundlevel'] < 30, 'ultrasoundlevel'] = 30\n",
    "\n",
    "# =======================\n",
    "# === Préparation Data ===\n",
    "# =======================\n",
    "# Sauvegarder l’ordre initial\n",
    "initial_order = df.index.copy()\n",
    "\n",
    "# Trier par datetime croissant\n",
    "df_sorted = df.sort_index()\n",
    "\n",
    "# ===============================\n",
    "# === Nettoyage autour des NaN ===\n",
    "# ===============================\n",
    "cols_suppl = df_sorted.columns.difference(['Label'])\n",
    "df_temp = df_sorted.reset_index()\n",
    "time_diffs = df_temp['time'].diff().dt.total_seconds()\n",
    "jump_positions = time_diffs[time_diffs > 1].index\n",
    "mask_zero = pd.Series(False, index=df_temp.index)\n",
    "for pos in jump_positions:\n",
    "    # ligne avant le saut (pos - 1) si possible\n",
    "    if pos > 0:\n",
    "        mask_zero.iloc[pos - 1] = True\n",
    "    # ligne du saut (pos)\n",
    "    mask_zero.iloc[pos] = True\n",
    "    # ligne après le saut (pos + 1) si possible\n",
    "    if pos < len(df_temp) - 1:\n",
    "        mask_zero.iloc[pos + 1] = True\n",
    "df_temp.loc[mask_zero, cols_suppl] = 0\n",
    "df_sorted = df_temp.set_index('time')\n",
    "\n",
    "# ============================================================================\n",
    "# === Extension des labels d’impacts lisser les imprécisions d'annotation  ===\n",
    "# ============================================================================\n",
    "df_src = df_sorted.reset_index()\n",
    "df_out = df_src.copy()\n",
    "\n",
    "# Identifier les lignes avec labels \"impact\" (≠ 'rotation', 'other')\n",
    "target_mask = ~df_src['Label'].isin(['rotation', 'other'])\n",
    "positions = df_src[target_mask].index\n",
    "\n",
    "# Étendre les annotations avant les impacts\n",
    "for pos in positions:\n",
    "    label = df_src.loc[pos, 'Label']\n",
    "    for offset in [-2,-1]:\n",
    "        new_pos = pos + offset\n",
    "        if 0 <= new_pos < len(df_out):\n",
    "            current = df_src.loc[new_pos, 'Label']\n",
    "            if current in ['rotation', 'other']:\n",
    "                df_out.loc[new_pos, 'Label'] = label\n",
    "\n",
    "df_modified = df_out.set_index('time')\n",
    "df_sorted = df_modified.loc[initial_order]\n",
    "\n",
    "# =========================================\n",
    "# === Attribution des couleurs par label ==\n",
    "# =========================================\n",
    "\n",
    "excluded_labels = ['rotation', 'other']\n",
    "valid_labels = [lbl for lbl in df['Label'].unique() if lbl not in excluded_labels]\n",
    "\n",
    "# Palette commune\n",
    "color_palette = px.colors.qualitative.Set2\n",
    "label_colors = {label: color_palette[i % len(color_palette)] for i, label in enumerate(valid_labels)}\n",
    "\n",
    "# Couleur spécifique pour \"other\"\n",
    "label_colors['other'] = 'rgba(0,0,0,1)'  # noir opaque\n",
    "\n",
    "# ==============================\n",
    "# === Détection des impacts ===\n",
    "# ==============================\n",
    "# Tri + suppression des doublons\n",
    "df_sorted = df_sorted.sort_index().copy()\n",
    "df_sorted = df_sorted[~df_sorted.index.duplicated(keep='first')]\n",
    "\n",
    "results = df_sorted[df_sorted['ultrasoundlevel']>0].copy()\n",
    "\n",
    "if features_to_display == 'all':\n",
    "    features_to_display = feature_list\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "time_array = results.index  # ou df['timestamp'] si c'est une colonne\n",
    "\n",
    "block_duration = (time_array[1] - time_array[0]).total_seconds()\n",
    "\n",
    "if batch == 'train':\n",
    "\n",
    "    # filtrer les lignes sans 'rivets' ni 'cutter'\n",
    "    mask = ~results['Label'].str.contains(\"rivets|cutter|rivets (pas d'impact)\", case=False, na=False)\n",
    "    filtered = results[mask].copy()\n",
    "    numeric_cols = filtered.select_dtypes(include='number').columns\n",
    "    data_scaled_array = scaler.fit_transform(filtered[numeric_cols])\n",
    "\n",
    "    # Sauvegarder les coefficients de normalisation\n",
    "    coefficients_df = pd.DataFrame({\n",
    "        'mean': scaler.mean_,\n",
    "        'scale': scaler.scale_\n",
    "    })\n",
    "    os.makedirs(f'results/{machine_name}', exist_ok=True)\n",
    "    coefficients_df.to_csv(f'results/{machine_name}/normalization_coefficients.csv', index=False)\n",
    "\n",
    "    data_scaled = pd.DataFrame(data_scaled_array, columns=numeric_cols, index=filtered.index)\n",
    "\n",
    "    valid_data = data_scaled[~np.isnan(data_scaled).any(axis=1)]  # Exclure les lignes contenant des NaN\n",
    "    data_scaled['Label'] = filtered['Label']\n",
    "\n",
    "    pca = PCA(n_components=2)  # Choisir le nombre de composantes principales\n",
    "    pca_results = pca.fit_transform(valid_data)  \n",
    "\n",
    "    loadings = pca.components_.T  \n",
    "    impact_pc1 = np.abs(loadings[:,0])  # Impact pour la 1ère composante principale\n",
    "    impact_pc2 = np.abs(loadings[:,1])  # Impact pour la 2ème composante principale\n",
    "\n",
    "    # Somme des impacts pour chaque paramètre (en termes d'importance totale dans la projection)\n",
    "    contributions = impact_pc1 + impact_pc2\n",
    "\n",
    "    contributions = pd.DataFrame({\n",
    "        'Feature': feature_list,\n",
    "        'Contributions': contributions\n",
    "    })\n",
    "    contributions = contributions.sort_values(by='Contributions', ascending=False)\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    labels = kmeans.fit_predict(pca_results)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    ellipses = calculate_ellipses(pca_results, labels, centroids, n_clusters,x_thresh,y_thresh)\n",
    "\n",
    "    with open(f'results/{machine_name}/ellipses.pkl', 'wb') as file:\n",
    "        pickle.dump(ellipses, file)\n",
    "\n",
    "    # Sauvegarder la matrice de transformation PCA\n",
    "    np.save(f'results/{machine_name}/pca_components.npy', pca.components_)  # Sauvegarder les vecteurs propres PCA\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    mask = results['Label'].str.contains(\"rivets|cutter|rivets (pas d'impact)\", case=False, na=False)\n",
    "    filtered = results[mask].copy()\n",
    "    numeric_cols = filtered.select_dtypes(include='number').columns\n",
    "\n",
    "    # Charger les coefficients de normalisation\n",
    "    coefficients_df = pd.read_csv(f'results/{machine_name}/normalization_coefficients.csv')\n",
    "    scaler.mean_ = coefficients_df['mean'].values\n",
    "    scaler.scale_ = coefficients_df['scale'].values\n",
    "\n",
    "    # Appliquer le scaler sur les données de test\n",
    "    if filtered.shape[0] == 0:\n",
    "        raise ValueError(\"No data to scale. Verify input to StandardScaler.\")\n",
    "    data_scaled_array = scaler.transform(filtered[numeric_cols])\n",
    "\n",
    "    # Charger les vecteurs propres PCA depuis le fichier\n",
    "    pca_components = np.load(f'results/{machine_name}/pca_components.npy')  # Charger les vecteurs propres PCA\n",
    "    contributions = None\n",
    "    data_scaled = pd.DataFrame(data_scaled_array, columns=numeric_cols, index=filtered.index)\n",
    "\n",
    "    valid_data = data_scaled[~np.isnan(data_scaled).any(axis=1)]  # Exclure les lignes contenant des NaN\n",
    "\n",
    "\n",
    "    # Étape 5 : ajouter la colonne 'Label' non modifiée\n",
    "    data_scaled['Label'] = filtered['Label']\n",
    "    pca_results = np.dot(valid_data, pca_components.T)\n",
    "    pca_with_nan = np.full((data_scaled.shape[0], pca_results.shape[1]), np.nan)\n",
    "    valid_mask = ~np.isnan(data_scaled_array).any(axis=1)  # Masque pour lignes valides (sans NaN)\n",
    "    pca_with_nan[valid_mask] = pca_results  # Remplir avec les résultats PCA\n",
    "\n",
    "    with open(f'results/{machine_name}/ellipses.pkl', 'rb') as file:\n",
    "        ellipses = pickle.load(file)\n",
    "\n",
    "\n",
    "raw_percentiles, _ = build_percentile_labels(filtered, batch=batch,machine_name=machine_name,percentile_filename='raw_percentiles.csv')\n",
    "standardized_percentiles, labels_percentile = build_percentile_labels(data_scaled, batch=batch,machine_name=machine_name,percentile_filename='standardized_percentiles.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f88d3",
   "metadata": {},
   "source": [
    "## Save & Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch == 'train':\n",
    "    save_percentiles_to_csv(raw_percentiles,machine_name, 'raw_percentiles.csv')    \n",
    "    save_percentiles_to_csv(standardized_percentiles,machine_name, 'standardized_percentiles.csv')\n",
    "\n",
    "if features_to_display:\n",
    "    # Afficher les histogrammes des données brutes\n",
    "    results_hist = {key: value for key, value in filtered.items() if key in features_to_display}\n",
    "    results_flatten = {key: value.values.flatten() for key, value in results_hist.items()}\n",
    "    display_histograms(results_flatten, title_suffix=' (Raw Data)', percentiles=raw_percentiles)\n",
    "\n",
    "    # Afficher les histogrammes des données standardisées\n",
    "    filtered_data_scaled_dict = {key: value for key, value in data_scaled.items() if key in features_to_display}\n",
    "    data_scaled_dict_flatten = {key: value.values.flatten() for key, value in filtered_data_scaled_dict.items()}\n",
    "    display_histograms(data_scaled_dict_flatten, title_suffix=' (Standardized Data)', percentiles=standardized_percentiles)\n",
    "\n",
    "labels_pca, time,_,rolling_percentile,rolling_pca,convex_hulls,alarm = borders(pca_results, labels_percentile,ellipses,rolling_window,block_duration,threshold,batch)  \n",
    "\n",
    "excluded_labels = ['rotation', 'other']\n",
    "valid_labels = [lbl for lbl in results['Label'].unique() if lbl not in excluded_labels]\n",
    "\n",
    "# Palette commune\n",
    "color_palette = px.colors.qualitative.Set2\n",
    "label_colors = {label: color_palette[i % len(color_palette)] for i, label in enumerate(valid_labels)}\n",
    "\n",
    "\n",
    "display_rolling_average(rolling_percentile,rolling_pca,time,alarm)\n",
    "\n",
    "display_pca(pca_results,convex_hulls,data_scaled,label_colors)\n",
    "\n",
    "if batch == 'test':\n",
    "    display_anomaly_detector(results,filtered,labels_pca,label_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py3.10-safran-nacelles-fod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
